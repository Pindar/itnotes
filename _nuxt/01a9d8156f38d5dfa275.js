(window.webpackJsonp=window.webpackJsonp||[]).push([[58],{248:function(e,n){e.exports={body:'Update: [In a new blog post I describe a better way to do the service discovery part](http://www.itnotes.de/coreos/deployment/tools/infrastructure/aws/docker/fleet/discovery/2015/06/14/service-discovery-and-log-shipping-with-docker/).\n\nCentralized logging is a key feature of a modern platform architecture. Obviously CoreOS brings some CLI tools to get logs but usually you are not in the lucky position to have only a coreos cluster running. Instead you probably have to manage a lot of different applications on different systems.\n\nMy goal was to setup a centralized log system quickly and without writing too many custom scripts. To achieve these two goals I tried Opsworks, Saltstack and in the end coreos. And I have to admit that the simplest approach for me was to use a coreos cluster. In this blog post I like to guide you through the setup step by step and also explain the log shipping system briefly.\n\nYou can find all the code on github in my [coreos-demo project](https://github.com/Pindar/coreos-demo).\n\n## Log shipping system overview\n\nI thought the best approach to handle a tremendously amount of log messages is to use syslog as shipping mechanism and elasticsearch together with kibana for visualization.\n\n![Log shipping overview](/log-shipping.png)\n\nEach application will ship the logs to a central rsyslog server by using the syslog protocol. The good thing is that almost every application logger system supports syslog (log4j, monolog etc.). In case you have application container (container with only one application running) you can also use a separate docker container that ships the logs. The important thing is to tag each log entry for simpler log parsing in the next step. Now the central rsyslog server can do both parse each log message and forward each log message to elasticsearch. Finally you can visualize and search through the log entries with kibana.\n\n## Hands On\n\nLet\'s setup the following:\n\n- running rsyslog service\n- make rsyslog available through a private route53 dns entry (e.g. syslog.foobar.local)\n- running elasticsearch cluster\n- running kibana which points to the elasticsearch cluster\n- make the kibana service available through a private route53 dns entry (e.g. kibana.foobar.local)\n\n### Elasticsearch\n\nLet\'s start with the elasticsearch cluster because both rsyslog and kibana depend on them. Luckily you can find an official ES image on dockerhub which means the only thing we need to do is writing the required systemd files. As reference implementation I found [the instructions of Matt Wright](http://mattupstate.com/coreos/devops/2014/06/26/running-an-elasticsearch-cluster-on-coreos.html)\n\nI\'m almost using Matt\'s approach. Only I\'m using my own [announcement mechanism](https://github.com/Pindar/docker-announcement-health-service) which does health checking in addition and mounting the data to a different volume on the host. In the best case this volume is a EBS volume and gets attached to the instance through another service at runtime. But for simplicity I skip this part.\n\n<pre>\n<code class="ini">\n[Unit]\nDescription=ElasticSearch service\nAfter=docker.service\n\n[Service]\nTimeoutSec=180\nEnvironmentFile=/etc/environment\n\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\n\nExecStart=/bin/bash -c \'\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\n  if [ "$?" = "0" ]; then \\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2); \\\n  else \\\n      UNICAST_HOSTS=""; \\\n  fi; \\\n  /usr/bin/docker run \\\n    --name %p-%i \\\n    -h `hostname` \\\n    --publish 9200:9200 \\\n    --publish 9300:9300 \\\n    --volume /vol/data/elasticsearch:/data \\\n    -e ES_HEAP_SIZE=512M \\\n    dockerfile/elasticsearch \\\n    /elasticsearch/bin/elasticsearch \\\n    --node.name=%p-%i \\\n    --cluster.name=logstash \\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\n    --discovery.zen.ping.multicast.enabled=false \\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\n\nExecStop=/usr/bin/docker stop %p-%i\nExecStop=/usr/bin/docker rm %p-%i\n\n[X-Fleet]\nConflicts=%p@*.service\n</code>\n</pre>\n\nSave this systemd configuration to `elasticsearch@.service` and submit it as systemd template to your CoreOS cluster by typing `fleetctl submit elasticsearch@.service`. At this point in time you can already start your elasticsearch cluster assuming your coreos cluster works and the required fleet metadata is set. Do so with `fleetctl start elasticsearch@1.service` and check the status with `fleetctl status elasticsearch@1.service`. To get the logs of our elasticsearch container you can use fleetctl again: `fleetctl journal elastichserach@1.service`.\n\nLet\'s announce the new service.\n<pre>\n<code class="ini">\n[Unit]\nDescription=ElasticSearch announce service\nAfter=elasticsearch@%i.service\nBindsTo=elasticsearch@%i.service\n\n[Service]\nEnvironmentFile=/etc/environment\n\nExecStartPre=-/usr/bin/docker kill elasticsearch-announce-%i\nExecStartPre=-/usr/bin/docker rm elasticsearch-announce-%i\nExecStartPre=/usr/bin/docker pull pindar/announcement-health-service\n\nExecStart=/bin/sh -c \'\\\n/usr/bin/docker run \\\n--name elasticsearch-announce-%i \\\n-e SERVICE=elasticsearch \\\n-e THRESHOLD=5 \\\n-e TIMEOUT=45 \\\n-e ENVIRONMENT=logs \\\n-e NUMBER=%i \\\n-e HOST_IP=${COREOS_PRIVATE_IPV4} \\\n-e HEALTH_URL=http://${COREOS_PRIVATE_IPV4}:9200 \\\n-e ANNOUNCE_VALUE=\\\'{"IP": "${COREOS_PRIVATE_IPV4}", "PORT": "9200", "TRANSPORT_PORT": "9300" }\\\' \\\npindar/announcement-health-service\'\n\nExecStop=/usr/bin/docker stop elasticsearch-announce-%i\n\n[X-Fleet]\nMachineOf=elasticsearch@%i.service\n</code>\n</pre>\n\nJust use the same fleetctl commands as above to start the announcement service.\n\n### ElasticSearch Client node\n\nTo have a [smart load balancing for your elasticseach cluster](http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html) you can easily start client nodes. Afterwards you can connect kibana or rsyslog to this nodes instead of using the cluster directly.\n\n<pre>\n<code class="ini">\n[Unit]\nDescription=ElasticSearch client service\nAfter=docker.service\n\n[Service]\nTimeoutSec=180\nEnvironmentFile=/etc/environment\n\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\n\nExecStart=/bin/bash -c \'\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\n  if [ "$?" = "0" ]; then \\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2); \\\n  else \\\n      UNICAST_HOSTS=""; \\\n  fi; \\\n  /usr/bin/docker run \\\n    --name %p-%i \\\n    -h `hostname` \\\n    --publish 9200:9200 \\\n    --publish 9300:9300 \\\n    --volume /vol/data/elasticsearch:/data \\\n    dockerfile/elasticsearch \\\n    /elasticsearch/bin/elasticsearch \\\n    --node.name=%p-%i \\\n    --node.data=false \\\n    --node.master=false \\\n    --cluster.name=logstash \\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\n    --discovery.zen.ping.multicast.enabled=false \\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\n\nExecStop=/usr/bin/docker stop %p-%i\nExecStop=/usr/bin/docker rm %p-%i\n\n[X-Fleet]\nConflicts=%p@*.service\n</code>\n</pre>\n\nYou can find all configuration files also on my github account in the [demo project](https://github.com/Pindar/coreos-demo) -- you need two announcement services for the elasticsearch client nodes: one that announces the node in the es-cluster and one that announces the node as client node for other services.\n\n### Kibana\n\nLet\'s connect Kibana to our new elasticsearch instance. You can find a [dockerized version of kibana 4 on my github repo](https://github.com/pindar/docker-kibana) and an automated build on [dockerhub](https://registry.hub.docker.com/u/pindar/kibana/).\n\nTo start a container from this image we need again the systemd configuration for our coreos cluster.\n\n<pre>\n<code class="ini">\n[Unit]\nDescription=Kibana logs front-end\nAfter=elasticsearch-announce@*.service\nRequires=elasticsearch-announce@*.service\n\n[Service]\nEnvironmentFile=/etc/environment\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill kibana\nExecStartPre=-/usr/bin/docker rm kibana\nExecStartPre=/usr/bin/docker pull pindar/kibana\n\nExecStart=/usr/bin/bash -c \'\\\ncurl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch-lb/logs/; \\\n  if [ "$?" = "0" ]; then \\\n      ELASTICSEARCH_ENDPOINT="http://$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2):9200"; \\\n  else \\\n      ELASTICSEARCH_ENDPOINT=""; \\\n  fi; \\\n/usr/bin/docker run \\\n--name kibana \\\n-e ELASTICSEARCH_ENDPOINT=$ELASTICSEARCH_ENDPOINT \\\n-p 5601:5601 \\\npindar/kibana\'\n\nExecStop=/usr/bin/docker stop kibana\n\n[X-Fleet]\nConflicts=kibana.service\n</code>\n</pre>\n\nAs soon as kibana is running you can try to connect to it on port 5601 and should see the interface.\n\n<img src="/kibana-first-start.png" width="100%" alt="Kibana after startup" />\n\nTo make it more convenient to access the UI you can dynamically update your DNS server. In case you are using AWS Route53 even for private entries you can use [my fork](https://github.com/Pindar/go-route53-presence) of an implementation that exactly does it for you. The fork was necessary because I don\'t wont to remove the dns entry every time fleet moves the service because it takes some time that the entry gets visible again (TTL of the SOA).\n\n<pre>\n<code class="ini">\n[Unit]\nDescription=kibana announcement service\n\nAfter=docker.service\nBindsTo=kibana.service\n\n[Service]\nExecStartPre=-/usr/bin/docker kill %p\nExecStartPre=-/usr/bin/docker rm %p\nExecStartPre=/usr/bin/docker pull pindar/go-route53-presence\n\nExecStart=/usr/bin/bash -c \\\n"/usr/bin/docker run \\\n--name %p \\\n-e AWS_ACCESS_KEY=`etcdctl get /AWS_USER_ROUTE53_KEY` \\\n-e AWS_SECRET_KEY=`etcdctl get /AWS_USER_ROUTE53_SECRET` \\\n-e ROUTE53_RECORD_NAME=logs.example.local. \\\n-e ROUTE53_RECORD_TYPE=\'A\' \\\n-e ROUTE53_TTL=15 \\\n-e ROUTE53_ZONE_ID=`etcdctl get /AWS_ROUTE53_ZONE_ID` \\\n-e ROUTE53_IP_TYPE=private \\\npindar/go-route53-presence"\n\nExecStop=/usr/bin/docker stop %p\n\n[X-Fleet]\nMachineOf=kibana.service\n</code>\n</pre>\n\n### Rsyslog\n\nThe last missing piece is the rsyslog container which will handle all the shipped logs. Even for this you can find a ready to use docker image on [dockerhub](https://registry.hub.docker.com/u/pindar/docker-rsyslog/)\n\n<pre>\n<code class="ini">\n[Unit]\nDescription=Centralised RSyslog\nAfter=docker.service\n\n[Service]\nUser=core\nTimeoutStartSec=0\nEnvironmentFile=/etc/environment\n\nExecStartPre=-/usr/bin/docker kill central-rsyslog-%i\nExecStartPre=-/usr/bin/docker rm central-rsyslog-%i\nExecStartPre=/usr/bin/docker pull pindar/docker-rsyslog\nExecStartPre=/usr/bin/sudo /usr/bin/mkdir -p /vol/logs\n\nExecStart=/usr/bin/bash -c \'\\\nELASTICSEARCH_ENDPOINT="$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2)"; \\\n/usr/bin/docker run \\\n--name central-rsyslog-%i \\\n-p 514:514 \\\n-p 514:514/udp \\\n-e ELASTICSEARCH_HOST=$ELASTICSEARCH_ENDPOINT \\\n-v /vol/logs:/var/log/remote \\\npindar/docker-rsyslog\'\n\nExecStop=/usr/bin/docker stop central-rsyslog-%i\n\n[X-Fleet]\nConflicts=central-rsyslog@*.service\n</code>\n</pre>\n\nTo test the round trip lookup the ip where rsyslog is running `fleetctl list-units` and connect with telnet to write your first log message.\n\n<pre>\n<code class="bash">\ntelnet 172.17.8.101 514\nTrying 172.17.8.101...\nConnected to 172.17.8.101.\nEscape character is \'^]\'.\ntest foobar : this is my message\n^]\ntelnet> quit\nConnection closed.\n</code>\n</pre>\n\n<img src="/kibana-log-messages.png" width="100%" alt="Kibana first log messages" />\n\nNow everything is up and running -- happy shipping!\n',html:'<p>Update: <a href="http://www.itnotes.de/coreos/deployment/tools/infrastructure/aws/docker/fleet/discovery/2015/06/14/service-discovery-and-log-shipping-with-docker/">In a new blog post I describe a better way to do the service discovery part</a>.</p>\n<p>Centralized logging is a key feature of a modern platform architecture. Obviously CoreOS brings some CLI tools to get logs but usually you are not in the lucky position to have only a coreos cluster running. Instead you probably have to manage a lot of different applications on different systems.</p>\n<p>My goal was to setup a centralized log system quickly and without writing too many custom scripts. To achieve these two goals I tried Opsworks, Saltstack and in the end coreos. And I have to admit that the simplest approach for me was to use a coreos cluster. In this blog post I like to guide you through the setup step by step and also explain the log shipping system briefly.</p>\n<p>You can find all the code on github in my <a href="https://github.com/Pindar/coreos-demo">coreos-demo project</a>.</p>\n<h2>Log shipping system overview</h2>\n<p>I thought the best approach to handle a tremendously amount of log messages is to use syslog as shipping mechanism and elasticsearch together with kibana for visualization.</p>\n<p><img src="/log-shipping.png" alt="Log shipping overview"></p>\n<p>Each application will ship the logs to a central rsyslog server by using the syslog protocol. The good thing is that almost every application logger system supports syslog (log4j, monolog etc.). In case you have application container (container with only one application running) you can also use a separate docker container that ships the logs. The important thing is to tag each log entry for simpler log parsing in the next step. Now the central rsyslog server can do both parse each log message and forward each log message to elasticsearch. Finally you can visualize and search through the log entries with kibana.</p>\n<h2>Hands On</h2>\n<p>Let\'s setup the following:</p>\n<ul>\n<li>running rsyslog service</li>\n<li>make rsyslog available through a private route53 dns entry (e.g. syslog.foobar.local)</li>\n<li>running elasticsearch cluster</li>\n<li>running kibana which points to the elasticsearch cluster</li>\n<li>make the kibana service available through a private route53 dns entry (e.g. kibana.foobar.local)</li>\n</ul>\n<h3>Elasticsearch</h3>\n<p>Let\'s start with the elasticsearch cluster because both rsyslog and kibana depend on them. Luckily you can find an official ES image on dockerhub which means the only thing we need to do is writing the required systemd files. As reference implementation I found <a href="http://mattupstate.com/coreos/devops/2014/06/26/running-an-elasticsearch-cluster-on-coreos.html">the instructions of Matt Wright</a></p>\n<p>I\'m almost using Matt\'s approach. Only I\'m using my own <a href="https://github.com/Pindar/docker-announcement-health-service">announcement mechanism</a> which does health checking in addition and mounting the data to a different volume on the host. In the best case this volume is a EBS volume and gets attached to the instance through another service at runtime. But for simplicity I skip this part.</p>\n<pre>\n<code class="ini">\n[Unit]\nDescription=ElasticSearch service\nAfter=docker.service\n\n[Service]\nTimeoutSec=180\nEnvironmentFile=/etc/environment\n\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\n\nExecStart=/bin/bash -c \'\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\n  if [ "$?" = "0" ]; then \\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2); \\\n  else \\\n      UNICAST_HOSTS=""; \\\n  fi; \\\n  /usr/bin/docker run \\\n    --name %p-%i \\\n    -h `hostname` \\\n    --publish 9200:9200 \\\n    --publish 9300:9300 \\\n    --volume /vol/data/elasticsearch:/data \\\n    -e ES_HEAP_SIZE=512M \\\n    dockerfile/elasticsearch \\\n    /elasticsearch/bin/elasticsearch \\\n    --node.name=%p-%i \\\n    --cluster.name=logstash \\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\n    --discovery.zen.ping.multicast.enabled=false \\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\n\nExecStop=/usr/bin/docker stop %p-%i\nExecStop=/usr/bin/docker rm %p-%i\n\n[X-Fleet]\nConflicts=%p@*.service\n</code>\n</pre>\n<p>Save this systemd configuration to <code>elasticsearch@.service</code> and submit it as systemd template to your CoreOS cluster by typing <code>fleetctl submit elasticsearch@.service</code>. At this point in time you can already start your elasticsearch cluster assuming your coreos cluster works and the required fleet metadata is set. Do so with <code>fleetctl start elasticsearch@1.service</code> and check the status with <code>fleetctl status elasticsearch@1.service</code>. To get the logs of our elasticsearch container you can use fleetctl again: <code>fleetctl journal elastichserach@1.service</code>.</p>\n<p>Let\'s announce the new service.</p>\n<pre>\n<code class="ini">\n[Unit]\nDescription=ElasticSearch announce service\nAfter=elasticsearch@%i.service\nBindsTo=elasticsearch@%i.service\n\n[Service]\nEnvironmentFile=/etc/environment\n\nExecStartPre=-/usr/bin/docker kill elasticsearch-announce-%i\nExecStartPre=-/usr/bin/docker rm elasticsearch-announce-%i\nExecStartPre=/usr/bin/docker pull pindar/announcement-health-service\n\nExecStart=/bin/sh -c \'\\\n/usr/bin/docker run \\\n--name elasticsearch-announce-%i \\\n-e SERVICE=elasticsearch \\\n-e THRESHOLD=5 \\\n-e TIMEOUT=45 \\\n-e ENVIRONMENT=logs \\\n-e NUMBER=%i \\\n-e HOST_IP=${COREOS_PRIVATE_IPV4} \\\n-e HEALTH_URL=http://${COREOS_PRIVATE_IPV4}:9200 \\\n-e ANNOUNCE_VALUE=\\\'{"IP": "${COREOS_PRIVATE_IPV4}", "PORT": "9200", "TRANSPORT_PORT": "9300" }\\\' \\\npindar/announcement-health-service\'\n\nExecStop=/usr/bin/docker stop elasticsearch-announce-%i\n\n[X-Fleet]\nMachineOf=elasticsearch@%i.service\n</code>\n</pre>\n<p>Just use the same fleetctl commands as above to start the announcement service.</p>\n<h3>ElasticSearch Client node</h3>\n<p>To have a <a href="http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html">smart load balancing for your elasticseach cluster</a> you can easily start client nodes. Afterwards you can connect kibana or rsyslog to this nodes instead of using the cluster directly.</p>\n<pre>\n<code class="ini">\n[Unit]\nDescription=ElasticSearch client service\nAfter=docker.service\n\n[Service]\nTimeoutSec=180\nEnvironmentFile=/etc/environment\n\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\n\nExecStart=/bin/bash -c \'\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\n  if [ "$?" = "0" ]; then \\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2); \\\n  else \\\n      UNICAST_HOSTS=""; \\\n  fi; \\\n  /usr/bin/docker run \\\n    --name %p-%i \\\n    -h `hostname` \\\n    --publish 9200:9200 \\\n    --publish 9300:9300 \\\n    --volume /vol/data/elasticsearch:/data \\\n    dockerfile/elasticsearch \\\n    /elasticsearch/bin/elasticsearch \\\n    --node.name=%p-%i \\\n    --node.data=false \\\n    --node.master=false \\\n    --cluster.name=logstash \\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\n    --discovery.zen.ping.multicast.enabled=false \\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\n\nExecStop=/usr/bin/docker stop %p-%i\nExecStop=/usr/bin/docker rm %p-%i\n\n[X-Fleet]\nConflicts=%p@*.service\n</code>\n</pre>\n<p>You can find all configuration files also on my github account in the <a href="https://github.com/Pindar/coreos-demo">demo project</a> -- you need two announcement services for the elasticsearch client nodes: one that announces the node in the es-cluster and one that announces the node as client node for other services.</p>\n<h3>Kibana</h3>\n<p>Let\'s connect Kibana to our new elasticsearch instance. You can find a <a href="https://github.com/pindar/docker-kibana">dockerized version of kibana 4 on my github repo</a> and an automated build on <a href="https://registry.hub.docker.com/u/pindar/kibana/">dockerhub</a>.</p>\n<p>To start a container from this image we need again the systemd configuration for our coreos cluster.</p>\n<pre>\n<code class="ini">\n[Unit]\nDescription=Kibana logs front-end\nAfter=elasticsearch-announce@*.service\nRequires=elasticsearch-announce@*.service\n\n[Service]\nEnvironmentFile=/etc/environment\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill kibana\nExecStartPre=-/usr/bin/docker rm kibana\nExecStartPre=/usr/bin/docker pull pindar/kibana\n\nExecStart=/usr/bin/bash -c \'\\\ncurl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch-lb/logs/; \\\n  if [ "$?" = "0" ]; then \\\n      ELASTICSEARCH_ENDPOINT="http://$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2):9200"; \\\n  else \\\n      ELASTICSEARCH_ENDPOINT=""; \\\n  fi; \\\n/usr/bin/docker run \\\n--name kibana \\\n-e ELASTICSEARCH_ENDPOINT=$ELASTICSEARCH_ENDPOINT \\\n-p 5601:5601 \\\npindar/kibana\'\n\nExecStop=/usr/bin/docker stop kibana\n\n[X-Fleet]\nConflicts=kibana.service\n</code>\n</pre>\n<p>As soon as kibana is running you can try to connect to it on port 5601 and should see the interface.</p>\n<img src="/kibana-first-start.png" width="100%" alt="Kibana after startup" />\n<p>To make it more convenient to access the UI you can dynamically update your DNS server. In case you are using AWS Route53 even for private entries you can use <a href="https://github.com/Pindar/go-route53-presence">my fork</a> of an implementation that exactly does it for you. The fork was necessary because I don\'t wont to remove the dns entry every time fleet moves the service because it takes some time that the entry gets visible again (TTL of the SOA).</p>\n<pre>\n<code class="ini">\n[Unit]\nDescription=kibana announcement service\n\nAfter=docker.service\nBindsTo=kibana.service\n\n[Service]\nExecStartPre=-/usr/bin/docker kill %p\nExecStartPre=-/usr/bin/docker rm %p\nExecStartPre=/usr/bin/docker pull pindar/go-route53-presence\n\nExecStart=/usr/bin/bash -c \\\n"/usr/bin/docker run \\\n--name %p \\\n-e AWS_ACCESS_KEY=`etcdctl get /AWS_USER_ROUTE53_KEY` \\\n-e AWS_SECRET_KEY=`etcdctl get /AWS_USER_ROUTE53_SECRET` \\\n-e ROUTE53_RECORD_NAME=logs.example.local. \\\n-e ROUTE53_RECORD_TYPE=\'A\' \\\n-e ROUTE53_TTL=15 \\\n-e ROUTE53_ZONE_ID=`etcdctl get /AWS_ROUTE53_ZONE_ID` \\\n-e ROUTE53_IP_TYPE=private \\\npindar/go-route53-presence"\n\nExecStop=/usr/bin/docker stop %p\n\n[X-Fleet]\nMachineOf=kibana.service\n</code>\n</pre>\n<h3>Rsyslog</h3>\n<p>The last missing piece is the rsyslog container which will handle all the shipped logs. Even for this you can find a ready to use docker image on <a href="https://registry.hub.docker.com/u/pindar/docker-rsyslog/">dockerhub</a></p>\n<pre>\n<code class="ini">\n[Unit]\nDescription=Centralised RSyslog\nAfter=docker.service\n\n[Service]\nUser=core\nTimeoutStartSec=0\nEnvironmentFile=/etc/environment\n\nExecStartPre=-/usr/bin/docker kill central-rsyslog-%i\nExecStartPre=-/usr/bin/docker rm central-rsyslog-%i\nExecStartPre=/usr/bin/docker pull pindar/docker-rsyslog\nExecStartPre=/usr/bin/sudo /usr/bin/mkdir -p /vol/logs\n\nExecStart=/usr/bin/bash -c \'\\\nELASTICSEARCH_ENDPOINT="$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2)"; \\\n/usr/bin/docker run \\\n--name central-rsyslog-%i \\\n-p 514:514 \\\n-p 514:514/udp \\\n-e ELASTICSEARCH_HOST=$ELASTICSEARCH_ENDPOINT \\\n-v /vol/logs:/var/log/remote \\\npindar/docker-rsyslog\'\n\nExecStop=/usr/bin/docker stop central-rsyslog-%i\n\n[X-Fleet]\nConflicts=central-rsyslog@*.service\n</code>\n</pre>\n<p>To test the round trip lookup the ip where rsyslog is running <code>fleetctl list-units</code> and connect with telnet to write your first log message.</p>\n<pre>\n<code class="bash">\ntelnet 172.17.8.101 514\nTrying 172.17.8.101...\nConnected to 172.17.8.101.\nEscape character is \'^]\'.\ntest foobar : this is my message\n^]\ntelnet> quit\nConnection closed.\n</code>\n</pre>\n<img src="/kibana-log-messages.png" width="100%" alt="Kibana first log messages" />\n<p>Now everything is up and running -- happy shipping!</p>\n',attributes:{layout:"post",title:"Responsive infrastructure - How to setup rsyslog, elasticsearch and kibana on CoreOS and AWS (2)",date:"2015-04-03T00:00:00.000Z",categories:"coreos deployment tools infrastructure aws docker fleet",comments:!0,image:"log-shipping.png",_meta:{resourcePath:"/home/travis/build/Pindar/itnotes/contents/posts/2015-04-03-how-to-setup-rsyslog-elasticsearch-kibana-on-coreos.md"}},vue:{render:"return function render() { var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _vm._m(0) }",staticRenderFns:'return [function () { var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c(\'div\',{staticClass:"dynamicMarkdown"},[_c(\'p\',[_vm._v("Update: "),_c(\'a\',{attrs:{"href":"http://www.itnotes.de/coreos/deployment/tools/infrastructure/aws/docker/fleet/discovery/2015/06/14/service-discovery-and-log-shipping-with-docker/"}},[_vm._v("In a new blog post I describe a better way to do the service discovery part")]),_vm._v(".")]),_vm._v(" "),_c(\'p\',[_vm._v("Centralized logging is a key feature of a modern platform architecture. Obviously CoreOS brings some CLI tools to get logs but usually you are not in the lucky position to have only a coreos cluster running. Instead you probably have to manage a lot of different applications on different systems.")]),_vm._v(" "),_c(\'p\',[_vm._v("My goal was to setup a centralized log system quickly and without writing too many custom scripts. To achieve these two goals I tried Opsworks, Saltstack and in the end coreos. And I have to admit that the simplest approach for me was to use a coreos cluster. In this blog post I like to guide you through the setup step by step and also explain the log shipping system briefly.")]),_vm._v(" "),_c(\'p\',[_vm._v("You can find all the code on github in my "),_c(\'a\',{attrs:{"href":"https://github.com/Pindar/coreos-demo"}},[_vm._v("coreos-demo project")]),_vm._v(".")]),_vm._v(" "),_c(\'h2\',[_vm._v("Log shipping system overview")]),_vm._v(" "),_c(\'p\',[_vm._v("I thought the best approach to handle a tremendously amount of log messages is to use syslog as shipping mechanism and elasticsearch together with kibana for visualization.")]),_vm._v(" "),_c(\'p\',[_c(\'img\',{attrs:{"src":"/log-shipping.png","alt":"Log shipping overview"}})]),_vm._v(" "),_c(\'p\',[_vm._v("Each application will ship the logs to a central rsyslog server by using the syslog protocol. The good thing is that almost every application logger system supports syslog (log4j, monolog etc.). In case you have application container (container with only one application running) you can also use a separate docker container that ships the logs. The important thing is to tag each log entry for simpler log parsing in the next step. Now the central rsyslog server can do both parse each log message and forward each log message to elasticsearch. Finally you can visualize and search through the log entries with kibana.")]),_vm._v(" "),_c(\'h2\',[_vm._v("Hands On")]),_vm._v(" "),_c(\'p\',[_vm._v("Let\'s setup the following:")]),_vm._v(" "),_c(\'ul\',[_c(\'li\',[_vm._v("running rsyslog service")]),_vm._v(" "),_c(\'li\',[_vm._v("make rsyslog available through a private route53 dns entry (e.g. syslog.foobar.local)")]),_vm._v(" "),_c(\'li\',[_vm._v("running elasticsearch cluster")]),_vm._v(" "),_c(\'li\',[_vm._v("running kibana which points to the elasticsearch cluster")]),_vm._v(" "),_c(\'li\',[_vm._v("make the kibana service available through a private route53 dns entry (e.g. kibana.foobar.local)")])]),_vm._v(" "),_c(\'h3\',[_vm._v("Elasticsearch")]),_vm._v(" "),_c(\'p\',[_vm._v("Let\'s start with the elasticsearch cluster because both rsyslog and kibana depend on them. Luckily you can find an official ES image on dockerhub which means the only thing we need to do is writing the required systemd files. As reference implementation I found "),_c(\'a\',{attrs:{"href":"http://mattupstate.com/coreos/devops/2014/06/26/running-an-elasticsearch-cluster-on-coreos.html"}},[_vm._v("the instructions of Matt Wright")])]),_vm._v(" "),_c(\'p\',[_vm._v("I\'m almost using Matt\'s approach. Only I\'m using my own "),_c(\'a\',{attrs:{"href":"https://github.com/Pindar/docker-announcement-health-service"}},[_vm._v("announcement mechanism")]),_vm._v(" which does health checking in addition and mounting the data to a different volume on the host. In the best case this volume is a EBS volume and gets attached to the instance through another service at runtime. But for simplicity I skip this part.")]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"ini"}},[_vm._v("\\n[Unit]\\nDescription=ElasticSearch service\\nAfter=docker.service\\n\\n[Service]\\nTimeoutSec=180\\nEnvironmentFile=/etc/environment\\n\\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\\n\\nExecStart=/bin/bash -c \'\\\\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\\\\n  if [ \\"$?\\" = \\"0\\" ]; then \\\\\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\\\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\\\'/:/ { print $2 }\\\\\' | cut -d\\\\\'\\"\\\\\' -f 2); \\\\\\n  else \\\\\\n      UNICAST_HOSTS=\\"\\"; \\\\\\n  fi; \\\\\\n  /usr/bin/docker run \\\\\\n    --name %p-%i \\\\\\n    -h `hostname` \\\\\\n    --publish 9200:9200 \\\\\\n    --publish 9300:9300 \\\\\\n    --volume /vol/data/elasticsearch:/data \\\\\\n    -e ES_HEAP_SIZE=512M \\\\\\n    dockerfile/elasticsearch \\\\\\n    /elasticsearch/bin/elasticsearch \\\\\\n    --node.name=%p-%i \\\\\\n    --cluster.name=logstash \\\\\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\\\\n    --discovery.zen.ping.multicast.enabled=false \\\\\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\\n\\nExecStop=/usr/bin/docker stop %p-%i\\nExecStop=/usr/bin/docker rm %p-%i\\n\\n[X-Fleet]\\nConflicts=%p@*.service\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'p\',[_vm._v("Save this systemd configuration to "),_c(\'code\',{pre:true},[_vm._v("elasticsearch@.service")]),_vm._v(" and submit it as systemd template to your CoreOS cluster by typing "),_c(\'code\',{pre:true},[_vm._v("fleetctl submit elasticsearch@.service")]),_vm._v(". At this point in time you can already start your elasticsearch cluster assuming your coreos cluster works and the required fleet metadata is set. Do so with "),_c(\'code\',{pre:true},[_vm._v("fleetctl start elasticsearch@1.service")]),_vm._v(" and check the status with "),_c(\'code\',{pre:true},[_vm._v("fleetctl status elasticsearch@1.service")]),_vm._v(". To get the logs of our elasticsearch container you can use fleetctl again: "),_c(\'code\',{pre:true},[_vm._v("fleetctl journal elastichserach@1.service")]),_vm._v(".")]),_vm._v(" "),_c(\'p\',[_vm._v("Let\'s announce the new service.")]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"ini"}},[_vm._v("\\n[Unit]\\nDescription=ElasticSearch announce service\\nAfter=elasticsearch@%i.service\\nBindsTo=elasticsearch@%i.service\\n\\n[Service]\\nEnvironmentFile=/etc/environment\\n\\nExecStartPre=-/usr/bin/docker kill elasticsearch-announce-%i\\nExecStartPre=-/usr/bin/docker rm elasticsearch-announce-%i\\nExecStartPre=/usr/bin/docker pull pindar/announcement-health-service\\n\\nExecStart=/bin/sh -c \'\\\\\\n/usr/bin/docker run \\\\\\n--name elasticsearch-announce-%i \\\\\\n-e SERVICE=elasticsearch \\\\\\n-e THRESHOLD=5 \\\\\\n-e TIMEOUT=45 \\\\\\n-e ENVIRONMENT=logs \\\\\\n-e NUMBER=%i \\\\\\n-e HOST_IP=${COREOS_PRIVATE_IPV4} \\\\\\n-e HEALTH_URL=http://${COREOS_PRIVATE_IPV4}:9200 \\\\\\n-e ANNOUNCE_VALUE=\\\\\'{\\"IP\\": \\"${COREOS_PRIVATE_IPV4}\\", \\"PORT\\": \\"9200\\", \\"TRANSPORT_PORT\\": \\"9300\\" }\\\\\' \\\\\\npindar/announcement-health-service\'\\n\\nExecStop=/usr/bin/docker stop elasticsearch-announce-%i\\n\\n[X-Fleet]\\nMachineOf=elasticsearch@%i.service\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'p\',[_vm._v("Just use the same fleetctl commands as above to start the announcement service.")]),_vm._v(" "),_c(\'h3\',[_vm._v("ElasticSearch Client node")]),_vm._v(" "),_c(\'p\',[_vm._v("To have a "),_c(\'a\',{attrs:{"href":"http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html"}},[_vm._v("smart load balancing for your elasticseach cluster")]),_vm._v(" you can easily start client nodes. Afterwards you can connect kibana or rsyslog to this nodes instead of using the cluster directly.")]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"ini"}},[_vm._v("\\n[Unit]\\nDescription=ElasticSearch client service\\nAfter=docker.service\\n\\n[Service]\\nTimeoutSec=180\\nEnvironmentFile=/etc/environment\\n\\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\\n\\nExecStart=/bin/bash -c \'\\\\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\\\\n  if [ \\"$?\\" = \\"0\\" ]; then \\\\\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\\\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\\\'/:/ { print $2 }\\\\\' | cut -d\\\\\'\\"\\\\\' -f 2); \\\\\\n  else \\\\\\n      UNICAST_HOSTS=\\"\\"; \\\\\\n  fi; \\\\\\n  /usr/bin/docker run \\\\\\n    --name %p-%i \\\\\\n    -h `hostname` \\\\\\n    --publish 9200:9200 \\\\\\n    --publish 9300:9300 \\\\\\n    --volume /vol/data/elasticsearch:/data \\\\\\n    dockerfile/elasticsearch \\\\\\n    /elasticsearch/bin/elasticsearch \\\\\\n    --node.name=%p-%i \\\\\\n    --node.data=false \\\\\\n    --node.master=false \\\\\\n    --cluster.name=logstash \\\\\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\\\\n    --discovery.zen.ping.multicast.enabled=false \\\\\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\\n\\nExecStop=/usr/bin/docker stop %p-%i\\nExecStop=/usr/bin/docker rm %p-%i\\n\\n[X-Fleet]\\nConflicts=%p@*.service\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'p\',[_vm._v("You can find all configuration files also on my github account in the "),_c(\'a\',{attrs:{"href":"https://github.com/Pindar/coreos-demo"}},[_vm._v("demo project")]),_vm._v(" -- you need two announcement services for the elasticsearch client nodes: one that announces the node in the es-cluster and one that announces the node as client node for other services.")]),_vm._v(" "),_c(\'h3\',[_vm._v("Kibana")]),_vm._v(" "),_c(\'p\',[_vm._v("Let\'s connect Kibana to our new elasticsearch instance. You can find a "),_c(\'a\',{attrs:{"href":"https://github.com/pindar/docker-kibana"}},[_vm._v("dockerized version of kibana 4 on my github repo")]),_vm._v(" and an automated build on "),_c(\'a\',{attrs:{"href":"https://registry.hub.docker.com/u/pindar/kibana/"}},[_vm._v("dockerhub")]),_vm._v(".")]),_vm._v(" "),_c(\'p\',[_vm._v("To start a container from this image we need again the systemd configuration for our coreos cluster.")]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"ini"}},[_vm._v("\\n[Unit]\\nDescription=Kibana logs front-end\\nAfter=elasticsearch-announce@*.service\\nRequires=elasticsearch-announce@*.service\\n\\n[Service]\\nEnvironmentFile=/etc/environment\\nTimeoutStartSec=0\\nExecStartPre=-/usr/bin/docker kill kibana\\nExecStartPre=-/usr/bin/docker rm kibana\\nExecStartPre=/usr/bin/docker pull pindar/kibana\\n\\nExecStart=/usr/bin/bash -c \'\\\\\\ncurl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch-lb/logs/; \\\\\\n  if [ \\"$?\\" = \\"0\\" ]; then \\\\\\n      ELASTICSEARCH_ENDPOINT=\\"http://$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\\\'/:/ { print $2 }\\\\\' | cut -d\\\\\'\\"\\\\\' -f 2):9200\\"; \\\\\\n  else \\\\\\n      ELASTICSEARCH_ENDPOINT=\\"\\"; \\\\\\n  fi; \\\\\\n/usr/bin/docker run \\\\\\n--name kibana \\\\\\n-e ELASTICSEARCH_ENDPOINT=$ELASTICSEARCH_ENDPOINT \\\\\\n-p 5601:5601 \\\\\\npindar/kibana\'\\n\\nExecStop=/usr/bin/docker stop kibana\\n\\n[X-Fleet]\\nConflicts=kibana.service\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'p\',[_vm._v("As soon as kibana is running you can try to connect to it on port 5601 and should see the interface.")]),_vm._v(" "),_c(\'img\',{attrs:{"src":"/kibana-first-start.png","width":"100%","alt":"Kibana after startup"}}),_vm._v(" "),_c(\'p\',[_vm._v("To make it more convenient to access the UI you can dynamically update your DNS server. In case you are using AWS Route53 even for private entries you can use "),_c(\'a\',{attrs:{"href":"https://github.com/Pindar/go-route53-presence"}},[_vm._v("my fork")]),_vm._v(" of an implementation that exactly does it for you. The fork was necessary because I don\'t wont to remove the dns entry every time fleet moves the service because it takes some time that the entry gets visible again (TTL of the SOA).")]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"ini"}},[_vm._v("\\n[Unit]\\nDescription=kibana announcement service\\n\\nAfter=docker.service\\nBindsTo=kibana.service\\n\\n[Service]\\nExecStartPre=-/usr/bin/docker kill %p\\nExecStartPre=-/usr/bin/docker rm %p\\nExecStartPre=/usr/bin/docker pull pindar/go-route53-presence\\n\\nExecStart=/usr/bin/bash -c \\\\\\n\\"/usr/bin/docker run \\\\\\n--name %p \\\\\\n-e AWS_ACCESS_KEY=`etcdctl get /AWS_USER_ROUTE53_KEY` \\\\\\n-e AWS_SECRET_KEY=`etcdctl get /AWS_USER_ROUTE53_SECRET` \\\\\\n-e ROUTE53_RECORD_NAME=logs.example.local. \\\\\\n-e ROUTE53_RECORD_TYPE=\'A\' \\\\\\n-e ROUTE53_TTL=15 \\\\\\n-e ROUTE53_ZONE_ID=`etcdctl get /AWS_ROUTE53_ZONE_ID` \\\\\\n-e ROUTE53_IP_TYPE=private \\\\\\npindar/go-route53-presence\\"\\n\\nExecStop=/usr/bin/docker stop %p\\n\\n[X-Fleet]\\nMachineOf=kibana.service\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'h3\',[_vm._v("Rsyslog")]),_vm._v(" "),_c(\'p\',[_vm._v("The last missing piece is the rsyslog container which will handle all the shipped logs. Even for this you can find a ready to use docker image on "),_c(\'a\',{attrs:{"href":"https://registry.hub.docker.com/u/pindar/docker-rsyslog/"}},[_vm._v("dockerhub")])]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"ini"}},[_vm._v("\\n[Unit]\\nDescription=Centralised RSyslog\\nAfter=docker.service\\n\\n[Service]\\nUser=core\\nTimeoutStartSec=0\\nEnvironmentFile=/etc/environment\\n\\nExecStartPre=-/usr/bin/docker kill central-rsyslog-%i\\nExecStartPre=-/usr/bin/docker rm central-rsyslog-%i\\nExecStartPre=/usr/bin/docker pull pindar/docker-rsyslog\\nExecStartPre=/usr/bin/sudo /usr/bin/mkdir -p /vol/logs\\n\\nExecStart=/usr/bin/bash -c \'\\\\\\nELASTICSEARCH_ENDPOINT=\\"$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\\\'/:/ { print $2 }\\\\\' | cut -d\\\\\'\\"\\\\\' -f 2)\\"; \\\\\\n/usr/bin/docker run \\\\\\n--name central-rsyslog-%i \\\\\\n-p 514:514 \\\\\\n-p 514:514/udp \\\\\\n-e ELASTICSEARCH_HOST=$ELASTICSEARCH_ENDPOINT \\\\\\n-v /vol/logs:/var/log/remote \\\\\\npindar/docker-rsyslog\'\\n\\nExecStop=/usr/bin/docker stop central-rsyslog-%i\\n\\n[X-Fleet]\\nConflicts=central-rsyslog@*.service\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'p\',[_vm._v("To test the round trip lookup the ip where rsyslog is running "),_c(\'code\',{pre:true},[_vm._v("fleetctl list-units")]),_vm._v(" and connect with telnet to write your first log message.")]),_vm._v(" "),_c(\'pre\',[_c(\'code\',{pre:true,attrs:{"class":"bash"}},[_vm._v("\\ntelnet 172.17.8.101 514\\nTrying 172.17.8.101...\\nConnected to 172.17.8.101.\\nEscape character is \'^]\'.\\ntest foobar : this is my message\\n^]\\ntelnet> quit\\nConnection closed.\\n")]),_vm._v("\\n")]),_vm._v(" "),_c(\'img\',{attrs:{"src":"/kibana-log-messages.png","width":"100%","alt":"Kibana first log messages"}}),_vm._v(" "),_c(\'p\',[_vm._v("Now everything is up and running -- happy shipping!")])]) }]',component:{data:function(){return{templateRender:null}},render:function(e){return this.templateRender?this.templateRender():e("div","Rendering")},created:function(){this.templateRender=function(){var e=this.$createElement;this._self._c;return this._m(0)},this.$options.staticRenderFns=[function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"dynamicMarkdown"},[t("p",[e._v("Update: "),t("a",{attrs:{href:"http://www.itnotes.de/coreos/deployment/tools/infrastructure/aws/docker/fleet/discovery/2015/06/14/service-discovery-and-log-shipping-with-docker/"}},[e._v("In a new blog post I describe a better way to do the service discovery part")]),e._v(".")]),e._v(" "),t("p",[e._v("Centralized logging is a key feature of a modern platform architecture. Obviously CoreOS brings some CLI tools to get logs but usually you are not in the lucky position to have only a coreos cluster running. Instead you probably have to manage a lot of different applications on different systems.")]),e._v(" "),t("p",[e._v("My goal was to setup a centralized log system quickly and without writing too many custom scripts. To achieve these two goals I tried Opsworks, Saltstack and in the end coreos. And I have to admit that the simplest approach for me was to use a coreos cluster. In this blog post I like to guide you through the setup step by step and also explain the log shipping system briefly.")]),e._v(" "),t("p",[e._v("You can find all the code on github in my "),t("a",{attrs:{href:"https://github.com/Pindar/coreos-demo"}},[e._v("coreos-demo project")]),e._v(".")]),e._v(" "),t("h2",[e._v("Log shipping system overview")]),e._v(" "),t("p",[e._v("I thought the best approach to handle a tremendously amount of log messages is to use syslog as shipping mechanism and elasticsearch together with kibana for visualization.")]),e._v(" "),t("p",[t("img",{attrs:{src:"/log-shipping.png",alt:"Log shipping overview"}})]),e._v(" "),t("p",[e._v("Each application will ship the logs to a central rsyslog server by using the syslog protocol. The good thing is that almost every application logger system supports syslog (log4j, monolog etc.). In case you have application container (container with only one application running) you can also use a separate docker container that ships the logs. The important thing is to tag each log entry for simpler log parsing in the next step. Now the central rsyslog server can do both parse each log message and forward each log message to elasticsearch. Finally you can visualize and search through the log entries with kibana.")]),e._v(" "),t("h2",[e._v("Hands On")]),e._v(" "),t("p",[e._v("Let's setup the following:")]),e._v(" "),t("ul",[t("li",[e._v("running rsyslog service")]),e._v(" "),t("li",[e._v("make rsyslog available through a private route53 dns entry (e.g. syslog.foobar.local)")]),e._v(" "),t("li",[e._v("running elasticsearch cluster")]),e._v(" "),t("li",[e._v("running kibana which points to the elasticsearch cluster")]),e._v(" "),t("li",[e._v("make the kibana service available through a private route53 dns entry (e.g. kibana.foobar.local)")])]),e._v(" "),t("h3",[e._v("Elasticsearch")]),e._v(" "),t("p",[e._v("Let's start with the elasticsearch cluster because both rsyslog and kibana depend on them. Luckily you can find an official ES image on dockerhub which means the only thing we need to do is writing the required systemd files. As reference implementation I found "),t("a",{attrs:{href:"http://mattupstate.com/coreos/devops/2014/06/26/running-an-elasticsearch-cluster-on-coreos.html"}},[e._v("the instructions of Matt Wright")])]),e._v(" "),t("p",[e._v("I'm almost using Matt's approach. Only I'm using my own "),t("a",{attrs:{href:"https://github.com/Pindar/docker-announcement-health-service"}},[e._v("announcement mechanism")]),e._v(" which does health checking in addition and mounting the data to a different volume on the host. In the best case this volume is a EBS volume and gets attached to the instance through another service at runtime. But for simplicity I skip this part.")]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"ini"}},[e._v('\n[Unit]\nDescription=ElasticSearch service\nAfter=docker.service\n\n[Service]\nTimeoutSec=180\nEnvironmentFile=/etc/environment\n\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\n\nExecStart=/bin/bash -c \'\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\n  if [ "$?" = "0" ]; then \\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2); \\\n  else \\\n      UNICAST_HOSTS=""; \\\n  fi; \\\n  /usr/bin/docker run \\\n    --name %p-%i \\\n    -h `hostname` \\\n    --publish 9200:9200 \\\n    --publish 9300:9300 \\\n    --volume /vol/data/elasticsearch:/data \\\n    -e ES_HEAP_SIZE=512M \\\n    dockerfile/elasticsearch \\\n    /elasticsearch/bin/elasticsearch \\\n    --node.name=%p-%i \\\n    --cluster.name=logstash \\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\n    --discovery.zen.ping.multicast.enabled=false \\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\n\nExecStop=/usr/bin/docker stop %p-%i\nExecStop=/usr/bin/docker rm %p-%i\n\n[X-Fleet]\nConflicts=%p@*.service\n')]),e._v("\n")]),e._v(" "),t("p",[e._v("Save this systemd configuration to "),t("code",{pre:!0},[e._v("elasticsearch@.service")]),e._v(" and submit it as systemd template to your CoreOS cluster by typing "),t("code",{pre:!0},[e._v("fleetctl submit elasticsearch@.service")]),e._v(". At this point in time you can already start your elasticsearch cluster assuming your coreos cluster works and the required fleet metadata is set. Do so with "),t("code",{pre:!0},[e._v("fleetctl start elasticsearch@1.service")]),e._v(" and check the status with "),t("code",{pre:!0},[e._v("fleetctl status elasticsearch@1.service")]),e._v(". To get the logs of our elasticsearch container you can use fleetctl again: "),t("code",{pre:!0},[e._v("fleetctl journal elastichserach@1.service")]),e._v(".")]),e._v(" "),t("p",[e._v("Let's announce the new service.")]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"ini"}},[e._v('\n[Unit]\nDescription=ElasticSearch announce service\nAfter=elasticsearch@%i.service\nBindsTo=elasticsearch@%i.service\n\n[Service]\nEnvironmentFile=/etc/environment\n\nExecStartPre=-/usr/bin/docker kill elasticsearch-announce-%i\nExecStartPre=-/usr/bin/docker rm elasticsearch-announce-%i\nExecStartPre=/usr/bin/docker pull pindar/announcement-health-service\n\nExecStart=/bin/sh -c \'\\\n/usr/bin/docker run \\\n--name elasticsearch-announce-%i \\\n-e SERVICE=elasticsearch \\\n-e THRESHOLD=5 \\\n-e TIMEOUT=45 \\\n-e ENVIRONMENT=logs \\\n-e NUMBER=%i \\\n-e HOST_IP=${COREOS_PRIVATE_IPV4} \\\n-e HEALTH_URL=http://${COREOS_PRIVATE_IPV4}:9200 \\\n-e ANNOUNCE_VALUE=\\\'{"IP": "${COREOS_PRIVATE_IPV4}", "PORT": "9200", "TRANSPORT_PORT": "9300" }\\\' \\\npindar/announcement-health-service\'\n\nExecStop=/usr/bin/docker stop elasticsearch-announce-%i\n\n[X-Fleet]\nMachineOf=elasticsearch@%i.service\n')]),e._v("\n")]),e._v(" "),t("p",[e._v("Just use the same fleetctl commands as above to start the announcement service.")]),e._v(" "),t("h3",[e._v("ElasticSearch Client node")]),e._v(" "),t("p",[e._v("To have a "),t("a",{attrs:{href:"http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html"}},[e._v("smart load balancing for your elasticseach cluster")]),e._v(" you can easily start client nodes. Afterwards you can connect kibana or rsyslog to this nodes instead of using the cluster directly.")]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"ini"}},[e._v('\n[Unit]\nDescription=ElasticSearch client service\nAfter=docker.service\n\n[Service]\nTimeoutSec=180\nEnvironmentFile=/etc/environment\n\nExecStartPre=/usr/bin/mkdir -p /vol/data/elasticsearch\nExecStartPre=/usr/bin/docker pull dockerfile/elasticsearch\n\nExecStart=/bin/bash -c \'\\\n  curl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch/logs; \\\n  if [ "$?" = "0" ]; then \\\n      PEER_PATH=$(etcdctl ls /announce/services/elasticsearch/logs | head -1); \\\n      UNICAST_HOSTS=$(etcdctl get $PEER_PATH | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2); \\\n  else \\\n      UNICAST_HOSTS=""; \\\n  fi; \\\n  /usr/bin/docker run \\\n    --name %p-%i \\\n    -h `hostname` \\\n    --publish 9200:9200 \\\n    --publish 9300:9300 \\\n    --volume /vol/data/elasticsearch:/data \\\n    dockerfile/elasticsearch \\\n    /elasticsearch/bin/elasticsearch \\\n    --node.name=%p-%i \\\n    --node.data=false \\\n    --node.master=false \\\n    --cluster.name=logstash \\\n    --network.publish_host=${COREOS_PRIVATE_IPV4} \\\n    --discovery.zen.ping.multicast.enabled=false \\\n    --discovery.zen.ping.unicast.hosts=$UNICAST_HOSTS\'\n\nExecStop=/usr/bin/docker stop %p-%i\nExecStop=/usr/bin/docker rm %p-%i\n\n[X-Fleet]\nConflicts=%p@*.service\n')]),e._v("\n")]),e._v(" "),t("p",[e._v("You can find all configuration files also on my github account in the "),t("a",{attrs:{href:"https://github.com/Pindar/coreos-demo"}},[e._v("demo project")]),e._v(" -- you need two announcement services for the elasticsearch client nodes: one that announces the node in the es-cluster and one that announces the node as client node for other services.")]),e._v(" "),t("h3",[e._v("Kibana")]),e._v(" "),t("p",[e._v("Let's connect Kibana to our new elasticsearch instance. You can find a "),t("a",{attrs:{href:"https://github.com/pindar/docker-kibana"}},[e._v("dockerized version of kibana 4 on my github repo")]),e._v(" and an automated build on "),t("a",{attrs:{href:"https://registry.hub.docker.com/u/pindar/kibana/"}},[e._v("dockerhub")]),e._v(".")]),e._v(" "),t("p",[e._v("To start a container from this image we need again the systemd configuration for our coreos cluster.")]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"ini"}},[e._v('\n[Unit]\nDescription=Kibana logs front-end\nAfter=elasticsearch-announce@*.service\nRequires=elasticsearch-announce@*.service\n\n[Service]\nEnvironmentFile=/etc/environment\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill kibana\nExecStartPre=-/usr/bin/docker rm kibana\nExecStartPre=/usr/bin/docker pull pindar/kibana\n\nExecStart=/usr/bin/bash -c \'\\\ncurl -f ${COREOS_PRIVATE_IPV4}:4001/v2/keys/announce/services/elasticsearch-lb/logs/; \\\n  if [ "$?" = "0" ]; then \\\n      ELASTICSEARCH_ENDPOINT="http://$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\\'/:/ { print $2 }\\\' | cut -d\\\'"\\\' -f 2):9200"; \\\n  else \\\n      ELASTICSEARCH_ENDPOINT=""; \\\n  fi; \\\n/usr/bin/docker run \\\n--name kibana \\\n-e ELASTICSEARCH_ENDPOINT=$ELASTICSEARCH_ENDPOINT \\\n-p 5601:5601 \\\npindar/kibana\'\n\nExecStop=/usr/bin/docker stop kibana\n\n[X-Fleet]\nConflicts=kibana.service\n')]),e._v("\n")]),e._v(" "),t("p",[e._v("As soon as kibana is running you can try to connect to it on port 5601 and should see the interface.")]),e._v(" "),t("img",{attrs:{src:"/kibana-first-start.png",width:"100%",alt:"Kibana after startup"}}),e._v(" "),t("p",[e._v("To make it more convenient to access the UI you can dynamically update your DNS server. In case you are using AWS Route53 even for private entries you can use "),t("a",{attrs:{href:"https://github.com/Pindar/go-route53-presence"}},[e._v("my fork")]),e._v(" of an implementation that exactly does it for you. The fork was necessary because I don't wont to remove the dns entry every time fleet moves the service because it takes some time that the entry gets visible again (TTL of the SOA).")]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"ini"}},[e._v("\n[Unit]\nDescription=kibana announcement service\n\nAfter=docker.service\nBindsTo=kibana.service\n\n[Service]\nExecStartPre=-/usr/bin/docker kill %p\nExecStartPre=-/usr/bin/docker rm %p\nExecStartPre=/usr/bin/docker pull pindar/go-route53-presence\n\nExecStart=/usr/bin/bash -c \\\n\"/usr/bin/docker run \\\n--name %p \\\n-e AWS_ACCESS_KEY=`etcdctl get /AWS_USER_ROUTE53_KEY` \\\n-e AWS_SECRET_KEY=`etcdctl get /AWS_USER_ROUTE53_SECRET` \\\n-e ROUTE53_RECORD_NAME=logs.example.local. \\\n-e ROUTE53_RECORD_TYPE='A' \\\n-e ROUTE53_TTL=15 \\\n-e ROUTE53_ZONE_ID=`etcdctl get /AWS_ROUTE53_ZONE_ID` \\\n-e ROUTE53_IP_TYPE=private \\\npindar/go-route53-presence\"\n\nExecStop=/usr/bin/docker stop %p\n\n[X-Fleet]\nMachineOf=kibana.service\n")]),e._v("\n")]),e._v(" "),t("h3",[e._v("Rsyslog")]),e._v(" "),t("p",[e._v("The last missing piece is the rsyslog container which will handle all the shipped logs. Even for this you can find a ready to use docker image on "),t("a",{attrs:{href:"https://registry.hub.docker.com/u/pindar/docker-rsyslog/"}},[e._v("dockerhub")])]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"ini"}},[e._v("\n[Unit]\nDescription=Centralised RSyslog\nAfter=docker.service\n\n[Service]\nUser=core\nTimeoutStartSec=0\nEnvironmentFile=/etc/environment\n\nExecStartPre=-/usr/bin/docker kill central-rsyslog-%i\nExecStartPre=-/usr/bin/docker rm central-rsyslog-%i\nExecStartPre=/usr/bin/docker pull pindar/docker-rsyslog\nExecStartPre=/usr/bin/sudo /usr/bin/mkdir -p /vol/logs\n\nExecStart=/usr/bin/bash -c '\\\nELASTICSEARCH_ENDPOINT=\"$(etcdctl get /announce/services/elasticsearch-lb/logs/1 | awk \\'/:/ { print $2 }\\' | cut -d\\'\"\\' -f 2)\"; \\\n/usr/bin/docker run \\\n--name central-rsyslog-%i \\\n-p 514:514 \\\n-p 514:514/udp \\\n-e ELASTICSEARCH_HOST=$ELASTICSEARCH_ENDPOINT \\\n-v /vol/logs:/var/log/remote \\\npindar/docker-rsyslog'\n\nExecStop=/usr/bin/docker stop central-rsyslog-%i\n\n[X-Fleet]\nConflicts=central-rsyslog@*.service\n")]),e._v("\n")]),e._v(" "),t("p",[e._v("To test the round trip lookup the ip where rsyslog is running "),t("code",{pre:!0},[e._v("fleetctl list-units")]),e._v(" and connect with telnet to write your first log message.")]),e._v(" "),t("pre",[t("code",{pre:!0,attrs:{class:"bash"}},[e._v("\ntelnet 172.17.8.101 514\nTrying 172.17.8.101...\nConnected to 172.17.8.101.\nEscape character is '^]'.\ntest foobar : this is my message\n^]\ntelnet> quit\nConnection closed.\n")]),e._v("\n")]),e._v(" "),t("img",{attrs:{src:"/kibana-log-messages.png",width:"100%",alt:"Kibana first log messages"}}),e._v(" "),t("p",[e._v("Now everything is up and running -- happy shipping!")])])}]}}}}}}]);